{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a0972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: imports and display options\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.width = 120\n",
    "pd.options.display.max_rows = 20\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbd5fa2",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data\n",
    "\n",
    "Let's recreate our cleaned, merged dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e51724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "media_df = pd.read_csv('../data/media_contacts.csv')\n",
    "demo_df = pd.read_csv('../data/socio_demos.csv')\n",
    "\n",
    "# Standardize column names\n",
    "media_df.columns = media_df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "demo_df.columns = demo_df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "\n",
    "print(f\"Media: {media_df.shape}\")\n",
    "print(f\"Demo: {demo_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5666336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets\n",
    "merged_df = pd.merge(media_df, demo_df, on='person_id', how='inner')\n",
    "\n",
    "# Basic cleaning\n",
    "merged_df['birthday_dt'] = pd.to_datetime(\n",
    "    merged_df['birthday'].astype(int).astype(str), \n",
    "    format='%Y%m%d',\n",
    "    errors='coerce'\n",
    ")\n",
    "merged_df['age'] = 2025 - merged_df['birthday_dt'].dt.year\n",
    "\n",
    "# Create age bands\n",
    "merged_df['age_band'] = pd.cut(\n",
    "    merged_df['age'],\n",
    "    bins=[0, 18, 25, 35, 45, 55, 65, 100],\n",
    "    labels=['<18', '18-24', '25-34', '35-44', '45-54', '55-64', '65+']\n",
    ")\n",
    "\n",
    "print(f\"Merged and cleaned: {merged_df.shape}\")\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48719cd",
   "metadata": {},
   "source": [
    "## 2. Create Output Directory\n",
    "\n",
    "Always create your output directory first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf17095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create outputs directory\n",
    "output_dir = '../outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {os.path.abspath(output_dir)}\")\n",
    "print(f\"Directory exists: {os.path.exists(output_dir)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045798ae",
   "metadata": {},
   "source": [
    "## 3. Export to CSV\n",
    "\n",
    "CSV is the most universal format - works everywhere.\n",
    "\n",
    "### Basic CSV Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e66de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic CSV export\n",
    "csv_path = f'{output_dir}/merged_data.csv'\n",
    "merged_df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Saved to: {csv_path}\")\n",
    "print(f\"File size: {os.path.getsize(csv_path) / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0995208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify we can load it back\n",
    "verify_df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"\\nLoaded back: {verify_df.shape}\")\n",
    "print(f\"Columns match: {list(merged_df.columns) == list(verify_df.columns)}\")\n",
    "print(f\"Data types preserved: {(merged_df.dtypes == verify_df.dtypes).sum()} / {len(merged_df.dtypes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e44c208",
   "metadata": {},
   "source": [
    "### CSV with Compression\n",
    "\n",
    "Compress CSVs to save space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fddbfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export with gzip compression\n",
    "csv_gz_path = f'{output_dir}/merged_data.csv.gz'\n",
    "merged_df.to_csv(csv_gz_path, index=False, compression='gzip')\n",
    "\n",
    "print(f\"Saved compressed CSV to: {csv_gz_path}\")\n",
    "print(f\"Original CSV: {os.path.getsize(csv_path) / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Compressed CSV: {os.path.getsize(csv_gz_path) / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Compression ratio: {os.path.getsize(csv_gz_path) / os.path.getsize(csv_path):.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bca438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas automatically handles gzip when reading\n",
    "verify_gz = pd.read_csv(csv_gz_path)\n",
    "\n",
    "print(f\"\\nLoaded compressed CSV: {verify_gz.shape}\")\n",
    "print(\"âœ… pandas automatically decompresses .csv.gz files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf4b0ce",
   "metadata": {},
   "source": [
    "### CSV Encoding and Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eb7df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTF-8 encoding (default, best practice)\n",
    "merged_df.to_csv(f'{output_dir}/data_utf8.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# UTF-8 with BOM (for Excel compatibility)\n",
    "merged_df.to_csv(f'{output_dir}/data_utf8_bom.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"Saved with different encodings:\")\n",
    "print(\"  - UTF-8: Best for most uses\")\n",
    "print(\"  - UTF-8-BOM: Best for opening in Excel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f2a886",
   "metadata": {},
   "source": [
    "## 4. Export to Parquet\n",
    "\n",
    "Parquet is a columnar format: faster to read, smaller size, preserves dtypes.\n",
    "\n",
    "### When to Use Parquet:\n",
    "âœ… Large datasets (>100MB)  \n",
    "âœ… Internal pipelines (Python to Python)  \n",
    "âœ… Need to preserve exact dtypes  \n",
    "âœ… Speed is important  \n",
    "\n",
    "### When to Use CSV:\n",
    "âœ… Small datasets  \n",
    "âœ… Sharing with non-technical users  \n",
    "âœ… Need to open in Excel  \n",
    "âœ… Maximum compatibility  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab8963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to Parquet\n",
    "parquet_path = f'{output_dir}/merged_data.parquet'\n",
    "merged_df.to_parquet(parquet_path, index=False, compression='snappy')\n",
    "\n",
    "print(f\"Saved to Parquet: {parquet_path}\")\n",
    "print(f\"\\nFile size comparison:\")\n",
    "print(f\"  CSV:         {os.path.getsize(csv_path) / 1024 / 1024:.2f} MB\")\n",
    "print(f\"  CSV.GZ:      {os.path.getsize(csv_gz_path) / 1024 / 1024:.2f} MB\")\n",
    "print(f\"  Parquet:     {os.path.getsize(parquet_path) / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce86413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Parquet\n",
    "verify_parquet = pd.read_parquet(parquet_path)\n",
    "\n",
    "print(f\"\\nLoaded from Parquet: {verify_parquet.shape}\")\n",
    "print(f\"\\nData types preserved:\")\n",
    "print(f\"Original dtypes == Parquet dtypes: {(merged_df.dtypes == verify_parquet.dtypes).all()}\")\n",
    "\n",
    "# Show datetime was preserved\n",
    "print(f\"\\nbirthday_dt preserved as datetime:\")\n",
    "print(f\"  Original: {merged_df['birthday_dt'].dtype}\")\n",
    "print(f\"  Parquet:  {verify_parquet['birthday_dt'].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b3c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet compression options\n",
    "compression_types = ['snappy', 'gzip', 'brotli']\n",
    "\n",
    "for comp in compression_types:\n",
    "    path = f'{output_dir}/merged_{comp}.parquet'\n",
    "    merged_df.to_parquet(path, index=False, compression=comp)\n",
    "    size_mb = os.path.getsize(path) / 1024 / 1024\n",
    "    print(f\"{comp:10s}: {size_mb:.2f} MB\")\n",
    "\n",
    "print(\"\\nRecommendation: Use 'snappy' for balanced speed/compression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b442201",
   "metadata": {},
   "source": [
    "## 5. Export Subsets of Data\n",
    "\n",
    "Often you need to save filtered data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f8433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export by gender\n",
    "males_df = merged_df[merged_df['gender'] == 'Male']\n",
    "females_df = merged_df[merged_df['gender'] == 'Female']\n",
    "\n",
    "males_df.to_csv(f'{output_dir}/males.csv', index=False)\n",
    "females_df.to_csv(f'{output_dir}/females.csv', index=False)\n",
    "\n",
    "print(f\"Saved gender subsets:\")\n",
    "print(f\"  Males:   {len(males_df):,} rows\")\n",
    "print(f\"  Females: {len(females_df):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906e15bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export by age band\n",
    "for age_band in merged_df['age_band'].cat.categories:\n",
    "    subset = merged_df[merged_df['age_band'] == age_band]\n",
    "    filename = f'{output_dir}/age_{age_band}.csv'.replace('<', 'under').replace('+', 'plus')\n",
    "    subset.to_csv(filename, index=False)\n",
    "    print(f\"Saved {age_band:8s}: {len(subset):,} rows -> {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b02892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export purchasers only\n",
    "purchasers_df = merged_df[merged_df['purchase'] == 1]\n",
    "purchasers_df.to_csv(f'{output_dir}/purchasers_only.csv', index=False)\n",
    "\n",
    "print(f\"\\nPurchasers only: {len(purchasers_df):,} rows\")\n",
    "print(f\"Purchase rate: {len(purchasers_df) / len(merged_df):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f0ee1b",
   "metadata": {},
   "source": [
    "## 6. Export Summary Tables\n",
    "\n",
    "Save aggregated analysis results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f2eef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary_table = merged_df.groupby(['gender', 'age_band'], observed=True).agg(\n",
    "    sample_size=('person_id', 'count'),\n",
    "    purchases=('purchase', 'sum'),\n",
    "    purchase_rate=('purchase', 'mean'),\n",
    "    avg_tv=('tv_total', 'mean'),\n",
    "    avg_online=('online_total', 'mean'),\n",
    "    avg_print=('print_total', 'mean')\n",
    ").round(4)\n",
    "\n",
    "print(\"Summary table:\")\n",
    "print(summary_table.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211ff339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary table\n",
    "summary_table.to_csv(f'{output_dir}/purchase_summary.csv')\n",
    "\n",
    "print(f\"Saved summary table with index\")\n",
    "print(f\"Rows: {len(summary_table)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a53de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index before saving (sometimes cleaner)\n",
    "summary_table_reset = summary_table.reset_index()\n",
    "summary_table_reset.to_csv(f'{output_dir}/purchase_summary_flat.csv', index=False)\n",
    "\n",
    "print(\"\\nSaved flattened summary table (no index)\")\n",
    "print(summary_table_reset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4d537e",
   "metadata": {},
   "source": [
    "## 7. Export to Excel\n",
    "\n",
    "Export to Excel with multiple sheets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f075559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single sheet Excel export\n",
    "excel_path = f'{output_dir}/merged_data.xlsx'\n",
    "merged_df.to_excel(excel_path, index=False, sheet_name='Data')\n",
    "\n",
    "print(f\"Saved to Excel: {excel_path}\")\n",
    "print(f\"File size: {os.path.getsize(excel_path) / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f259c372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple sheets in one Excel file\n",
    "with pd.ExcelWriter(f'{output_dir}/analysis_report.xlsx', engine='openpyxl') as writer:\n",
    "    # Main data\n",
    "    merged_df.head(1000).to_excel(writer, sheet_name='Sample Data', index=False)\n",
    "    \n",
    "    # Summary table\n",
    "    summary_table_reset.to_excel(writer, sheet_name='Purchase Summary', index=False)\n",
    "    \n",
    "    # Gender breakdown\n",
    "    gender_summary = merged_df.groupby('gender').agg({\n",
    "        'person_id': 'count',\n",
    "        'purchase': ['sum', 'mean'],\n",
    "        'tv_total': 'mean'\n",
    "    })\n",
    "    gender_summary.to_excel(writer, sheet_name='Gender Summary')\n",
    "    \n",
    "    # Age breakdown\n",
    "    age_summary = merged_df.groupby('age_band', observed=True).agg({\n",
    "        'person_id': 'count',\n",
    "        'purchase': 'mean'\n",
    "    })\n",
    "    age_summary.to_excel(writer, sheet_name='Age Summary')\n",
    "\n",
    "print(\"\\nSaved multi-sheet Excel workbook:\")\n",
    "print(\"  - Sample Data (first 1000 rows)\")\n",
    "print(\"  - Purchase Summary\")\n",
    "print(\"  - Gender Summary\")\n",
    "print(\"  - Age Summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d340b930",
   "metadata": {},
   "source": [
    "## 8. Export Selected Columns\n",
    "\n",
    "Save only the columns you need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0ddc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select key columns for sharing\n",
    "key_columns = [\n",
    "    'person_id', 'gender', 'age', 'age_band',\n",
    "    'tv_total', 'online_total', 'print_total',\n",
    "    'purchase', 'weight'\n",
    "]\n",
    "\n",
    "minimal_df = merged_df[key_columns]\n",
    "\n",
    "minimal_df.to_csv(f'{output_dir}/minimal_dataset.csv', index=False)\n",
    "\n",
    "print(f\"Saved minimal dataset:\")\n",
    "print(f\"  Original columns: {len(merged_df.columns)}\")\n",
    "print(f\"  Minimal columns:  {len(minimal_df.columns)}\")\n",
    "print(f\"  Original size: {os.path.getsize(csv_path) / 1024 / 1024:.2f} MB\")\n",
    "print(f\"  Minimal size:  {os.path.getsize(f'{output_dir}/minimal_dataset.csv') / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bf161e",
   "metadata": {},
   "source": [
    "## 9. Format Selection Guide\n",
    "\n",
    "Choose the right format for your use case:\n",
    "\n",
    "| Format | Best For | Pros | Cons |\n",
    "|--------|----------|------|------|\n",
    "| **CSV** | Sharing, Excel, small files | Universal, human-readable | Large size, loses dtypes |\n",
    "| **CSV.GZ** | Archival, medium files | Compressed, universal | Slower to read |\n",
    "| **Parquet** | Pipelines, large files, Python | Fast, small, preserves types | Not human-readable |\n",
    "| **Excel** | Reports, business users | Multiple sheets, formatting | Slow, size limits |\n",
    "| **Pickle** | Python only, exact state | Preserves everything | Python-specific, security risk |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb64e1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate format comparison\n",
    "formats = {\n",
    "    'CSV': (f'{output_dir}/format_test.csv', lambda df, path: df.to_csv(path, index=False)),\n",
    "    'CSV.GZ': (f'{output_dir}/format_test.csv.gz', lambda df, path: df.to_csv(path, index=False, compression='gzip')),\n",
    "    'Parquet': (f'{output_dir}/format_test.parquet', lambda df, path: df.to_parquet(path, index=False)),\n",
    "    'Excel': (f'{output_dir}/format_test.xlsx', lambda df, path: df.to_excel(path, index=False)),\n",
    "    'Pickle': (f'{output_dir}/format_test.pkl', lambda df, path: df.to_pickle(path))\n",
    "}\n",
    "\n",
    "test_df = merged_df.head(5000)  # Use subset for speed\n",
    "\n",
    "print(\"Format comparison (5,000 rows):\\n\")\n",
    "print(f\"{'Format':<12} {'Size (MB)':<12} {'Write Time':<15} {'Read Time'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "import time\n",
    "\n",
    "for format_name, (path, save_func) in formats.items():\n",
    "    # Write\n",
    "    start = time.time()\n",
    "    save_func(test_df, path)\n",
    "    write_time = time.time() - start\n",
    "    \n",
    "    # Get size\n",
    "    size_mb = os.path.getsize(path) / 1024 / 1024\n",
    "    \n",
    "    # Read\n",
    "    start = time.time()\n",
    "    if format_name == 'CSV' or format_name == 'CSV.GZ':\n",
    "        _ = pd.read_csv(path)\n",
    "    elif format_name == 'Parquet':\n",
    "        _ = pd.read_parquet(path)\n",
    "    elif format_name == 'Excel':\n",
    "        _ = pd.read_excel(path)\n",
    "    elif format_name == 'Pickle':\n",
    "        _ = pd.read_pickle(path)\n",
    "    read_time = time.time() - start\n",
    "    \n",
    "    print(f\"{format_name:<12} {size_mb:<12.2f} {write_time:<15.3f} {read_time:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0811ea68",
   "metadata": {},
   "source": [
    "## 10. Best Practices\n",
    "\n",
    "### File Naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3bca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include date in filename\n",
    "from datetime import datetime\n",
    "\n",
    "date_str = datetime.now().strftime('%Y%m%d')\n",
    "dated_path = f'{output_dir}/merged_data_{date_str}.csv'\n",
    "\n",
    "merged_df.to_csv(dated_path, index=False)\n",
    "print(f\"Saved with date: {dated_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ea0112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include metadata in filename\n",
    "version = 'v2'\n",
    "filter_type = 'all_ages'\n",
    "metric_path = f'{output_dir}/analysis_{version}_{filter_type}_{date_str}.csv'\n",
    "\n",
    "summary_table_reset.to_csv(metric_path, index=False)\n",
    "print(f\"Saved with metadata: {metric_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ae387",
   "metadata": {},
   "source": [
    "### Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe497e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data dictionary\n",
    "data_dict = pd.DataFrame({\n",
    "    'column_name': merged_df.columns,\n",
    "    'dtype': merged_df.dtypes.astype(str),\n",
    "    'non_null_count': merged_df.notnull().sum(),\n",
    "    'null_count': merged_df.isnull().sum(),\n",
    "    'unique_values': [merged_df[col].nunique() for col in merged_df.columns],\n",
    "    'sample_value': [str(merged_df[col].iloc[0]) if len(merged_df) > 0 else '' for col in merged_df.columns]\n",
    "})\n",
    "\n",
    "data_dict.to_csv(f'{output_dir}/data_dictionary.csv', index=False)\n",
    "\n",
    "print(\"Saved data dictionary:\")\n",
    "print(data_dict.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b612784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create README file\n",
    "readme_content = f\"\"\"# Data Export Summary\n",
    "\n",
    "**Export Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**Source Data**: media_contacts.csv, socio_demos.csv\n",
    "**Total Rows**: {len(merged_df):,}\n",
    "**Total Columns**: {len(merged_df.columns)}\n",
    "\n",
    "## Files Included\n",
    "\n",
    "1. **merged_data.csv** - Full merged dataset\n",
    "2. **merged_data.parquet** - Parquet format (faster)\n",
    "3. **merged_data.csv.gz** - Compressed CSV\n",
    "4. **males.csv** - Male respondents only\n",
    "5. **females.csv** - Female respondents only\n",
    "6. **purchase_summary.csv** - Aggregated purchase statistics\n",
    "7. **data_dictionary.csv** - Column descriptions\n",
    "\n",
    "## Column Descriptions\n",
    "\n",
    "- person_id: Unique identifier\n",
    "- gender: Male/Female\n",
    "- age: Age in years\n",
    "- age_band: Age category\n",
    "- tv_total: Total TV exposure minutes\n",
    "- online_total: Total online exposure minutes\n",
    "- print_total: Total print exposure minutes\n",
    "- purchase: Purchase indicator (0/1)\n",
    "- weight: Survey weight\n",
    "\n",
    "## Notes\n",
    "\n",
    "- Data cleaned and validated\n",
    "- Missing values handled\n",
    "- Categorical variables created\n",
    "- Ready for analysis\n",
    "\"\"\"\n",
    "\n",
    "with open(f'{output_dir}/README.txt', 'w') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"Saved README.txt with documentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a63251c",
   "metadata": {},
   "source": [
    "## 11. Reproducible Pipeline\n",
    "\n",
    "Create a complete save/load pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35059e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete export function\n",
    "def export_analysis_results(df, output_dir='../outputs', prefix='analysis'):\n",
    "    \"\"\"\n",
    "    Export analysis results in multiple formats with documentation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Data to export\n",
    "    output_dir : str\n",
    "        Output directory path\n",
    "    prefix : str\n",
    "        Filename prefix\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    date_str = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Export in multiple formats\n",
    "    base_path = f'{output_dir}/{prefix}_{date_str}'\n",
    "    \n",
    "    # CSV\n",
    "    df.to_csv(f'{base_path}.csv', index=False)\n",
    "    print(f\"âœ… Saved CSV: {base_path}.csv\")\n",
    "    \n",
    "    # Compressed CSV\n",
    "    df.to_csv(f'{base_path}.csv.gz', index=False, compression='gzip')\n",
    "    print(f\"âœ… Saved CSV.GZ: {base_path}.csv.gz\")\n",
    "    \n",
    "    # Parquet\n",
    "    df.to_parquet(f'{base_path}.parquet', index=False)\n",
    "    print(f\"âœ… Saved Parquet: {base_path}.parquet\")\n",
    "    \n",
    "    # Data dictionary\n",
    "    data_dict = pd.DataFrame({\n",
    "        'column': df.columns,\n",
    "        'dtype': df.dtypes.astype(str),\n",
    "        'non_null': df.notnull().sum(),\n",
    "        'unique': [df[col].nunique() for col in df.columns]\n",
    "    })\n",
    "    data_dict.to_csv(f'{base_path}_dictionary.csv', index=False)\n",
    "    print(f\"âœ… Saved data dictionary: {base_path}_dictionary.csv\")\n",
    "    \n",
    "    # Summary stats\n",
    "    summary = pd.DataFrame({\n",
    "        'metric': ['rows', 'columns', 'memory_mb'],\n",
    "        'value': [len(df), len(df.columns), df.memory_usage(deep=True).sum() / 1024 / 1024]\n",
    "    })\n",
    "    summary.to_csv(f'{base_path}_summary.csv', index=False)\n",
    "    print(f\"âœ… Saved summary: {base_path}_summary.csv\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ‰ Export complete! Files saved to: {output_dir}\")\n",
    "    return base_path\n",
    "\n",
    "# Use the function\n",
    "export_path = export_analysis_results(merged_df, output_dir, 'merged_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f124b737",
   "metadata": {},
   "source": [
    "## 12. Loading Data Back\n",
    "\n",
    "Best practices for loading saved data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f49f8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV\n",
    "loaded_csv = pd.read_csv(f'{output_dir}/merged_data.csv')\n",
    "\n",
    "# Specify dtypes for better performance\n",
    "dtype_spec = {\n",
    "    'person_id': 'int64',\n",
    "    'gender': 'category',\n",
    "    'purchase': 'int8',\n",
    "}\n",
    "\n",
    "loaded_csv_typed = pd.read_csv(\n",
    "    f'{output_dir}/merged_data.csv',\n",
    "    dtype=dtype_spec,\n",
    "    parse_dates=['birthday_dt']\n",
    ")\n",
    "\n",
    "print(\"Loaded CSV with specified dtypes:\")\n",
    "print(loaded_csv_typed.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e3aea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Parquet (dtypes preserved automatically)\n",
    "loaded_parquet = pd.read_parquet(f'{output_dir}/merged_data.parquet')\n",
    "\n",
    "print(\"\\nLoaded Parquet (dtypes auto-preserved):\")\n",
    "print(loaded_parquet.dtypes)\n",
    "print(f\"\\nData identical: {loaded_parquet.equals(merged_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5036607d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "âœ… Export to CSV with compression  \n",
    "âœ… Save to Parquet for performance  \n",
    "âœ… Choose the right file format  \n",
    "âœ… Export to Excel with multiple sheets  \n",
    "âœ… Save subsets and filtered data  \n",
    "âœ… Create data dictionaries and documentation  \n",
    "âœ… Build reproducible export pipelines  \n",
    "âœ… Follow best practices for file naming  \n",
    "âœ… Load data back efficiently  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **CSV for sharing**, Parquet for pipelines\n",
    "2. **Compress large CSVs** with gzip\n",
    "3. **Document your exports** with data dictionaries\n",
    "4. **Use date stamps** in filenames\n",
    "5. **Preserve dtypes** with Parquet or dtype specs\n",
    "6. **Test your exports** by loading them back\n",
    "7. **Create reusable functions** for consistent exports\n",
    "\n",
    "### Congratulations! ðŸŽ‰\n",
    "\n",
    "You've completed all 7 notebooks and learned:\n",
    "\n",
    "1. **Data Loading** - Read CSVs, inspect DataFrames\n",
    "2. **Selection & Indexing** - Filter and subset data\n",
    "3. **Cleaning & Transformations** - Handle messy data\n",
    "4. **Merging & Joining** - Combine datasets\n",
    "5. **GroupBy & Aggregation** - Summarize data\n",
    "6. **Reshaping & Pivoting** - Change data structure\n",
    "7. **Exporting & Saving** - Save results\n",
    "\n",
    "You now have the skills to:\n",
    "- Load and clean real-world data\n",
    "- Transform and merge multiple datasets\n",
    "- Perform complex aggregations\n",
    "- Reshape data for analysis\n",
    "- Export results professionally\n",
    "\n",
    "**Next Steps**: Apply these skills to your own data projects! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1572f8c",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Practice Exercises\n",
    "\n",
    "Try these on your own:\n",
    "\n",
    "1. Export only purchasers to a compressed CSV\n",
    "2. Save gender summary stats to Excel with formatting\n",
    "3. Create separate Parquet files for each age band\n",
    "4. Export top 10 most active users (by total media exposure) to CSV\n",
    "5. Create a multi-sheet Excel report with data + 3 summary tables\n",
    "6. Save a data dictionary for the minimal_dataset\n",
    "7. Create a function that exports a DataFrame with automatic compression based on size\n",
    "8. Load a CSV and validate it matches the original DataFrame\n",
    "\n",
    "### Bonus Challenges\n",
    "\n",
    "9. Create a complete export pipeline that saves data, summaries, and visualizations\n",
    "10. Write a function that compares file sizes across all formats and recommends the best one\n",
    "11. Create dated backup files (keep last 7 days, delete older)\n",
    "12. Export data with custom column order and subset of rows based on a filter\n",
    "\n",
    "### Ultimate Challenge\n",
    "\n",
    "Build a complete data processing script that:\n",
    "1. Loads raw data\n",
    "2. Cleans and transforms it\n",
    "3. Creates multiple analyses\n",
    "4. Exports everything to organized folders\n",
    "5. Creates a comprehensive README\n",
    "6. Includes error handling and logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6ba5ae",
   "metadata": {},
   "source": [
    "## Loading/Saving Data Between Notebooks\n",
    "\n",
    "### Load Previously Processed Data\n",
    "\n",
    "```python\n",
    "# Uncomment to load from previous notebooks\n",
    "# merged_df = pd.read_csv('../outputs/merged_data.csv')\n",
    "# \n",
    "# # Or load from Parquet (faster, preserves types)\n",
    "# merged_df = pd.read_parquet('../outputs/merged_data.parquet')\n",
    "#\n",
    "# # Load summary tables\n",
    "# summary_df = pd.read_csv('../outputs/purchase_summary_flat.csv')\n",
    "#\n",
    "# print(f\"Loaded data: {merged_df.shape}\")\n",
    "```\n",
    "\n",
    "### Final Export\n",
    "\n",
    "```python\n",
    "# This notebook IS about exporting!\n",
    "# All export code is in the cells above.\n",
    "# \n",
    "# Recommended final exports:\n",
    "# 1. Full merged dataset -> Parquet (fast access)\n",
    "# 2. Full merged dataset -> CSV.GZ (universal backup)\n",
    "# 3. Summary tables -> CSV (for reporting)\n",
    "# 4. Analysis report -> Excel (for stakeholders)\n",
    "# 5. Data dictionary -> CSV (for documentation)\n",
    "```\n",
    "\n",
    "### Archive Your Work\n",
    "\n",
    "```python\n",
    "# Uncomment to create a dated archive\n",
    "# import shutil\n",
    "# from datetime import datetime\n",
    "# \n",
    "# date_str = datetime.now().strftime('%Y%m%d')\n",
    "# archive_dir = f'../outputs/archive_{date_str}'\n",
    "# \n",
    "# # Copy outputs to archive\n",
    "# shutil.copytree(output_dir, archive_dir, dirs_exist_ok=True)\n",
    "# \n",
    "# print(f\"âœ… Archived outputs to: {archive_dir}\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
