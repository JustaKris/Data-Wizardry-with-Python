{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3c14ba1",
   "metadata": {},
   "source": [
    "## Contents\n",
    "- Column selection\n",
    "- Row selection with `.loc` and `.iloc`\n",
    "- Boolean masks and chaining vs. single-expression filters\n",
    "- Using `query()` for readable filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a561bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: imports and display options\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.width = 120\n",
    "pd.options.display.max_rows = 20\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc84864a",
   "metadata": {},
   "source": [
    "## 1. Load Raw Datasets\n",
    "\n",
    "We'll start fresh with the original data:\n",
    "\n",
    "**Note**: You could load cleaned data from notebook 03 (see commented section at end)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecfe37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load media contacts data\n",
    "media_df = pd.read_csv('../data/media_contacts.csv')\n",
    "\n",
    "print(f\"Media data: {media_df.shape}\")\n",
    "print(f\"Columns: {media_df.columns.tolist()}\")\n",
    "media_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b84490f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load demographic data\n",
    "demo_df = pd.read_csv('../data/socio_demos.csv')\n",
    "\n",
    "print(f\"Demo data: {demo_df.shape}\")\n",
    "print(f\"Columns: {demo_df.columns.tolist()}\")\n",
    "demo_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f13570",
   "metadata": {},
   "source": [
    "## 2. Harmonizing Column Names\n",
    "\n",
    "**Problem**: Notice the ID columns have different names:\n",
    "- Media: `PERSON ID` (with space)\n",
    "- Demo: `Person ID` (with space, different capitalization)\n",
    "\n",
    "Before merging, we need to standardize these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1657f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the exact column names\n",
    "print(\"Media ID column:\", [col for col in media_df.columns if 'ID' in col.upper()])\n",
    "print(\"Demo ID column:\", [col for col in demo_df.columns if 'ID' in col.upper()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac117f82",
   "metadata": {},
   "source": [
    "### Standardize All Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea9725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize media columns\n",
    "media_df.columns = media_df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "\n",
    "# Standardize demo columns  \n",
    "demo_df.columns = demo_df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "\n",
    "print(\"Standardized columns:\")\n",
    "print(f\"Media: {media_df.columns.tolist()}\")\n",
    "print(f\"\\nDemo: {demo_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a6ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify we now have matching ID columns\n",
    "print(\"\\nID columns after standardization:\")\n",
    "print(f\"Media: 'person_id' in columns = {'person_id' in media_df.columns}\")\n",
    "print(f\"Demo: 'person_id' in columns = {'person_id' in demo_df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d44b49",
   "metadata": {},
   "source": [
    "## 3. Understanding Merge Types\n",
    "\n",
    "pandas `merge()` works like SQL joins. Let's explore each type:\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **left**: Keep all rows from left DataFrame, match from right\n",
    "- **right**: Keep all rows from right DataFrame, match from left  \n",
    "- **inner**: Keep only rows that match in both DataFrames\n",
    "- **outer**: Keep all rows from both DataFrames\n",
    "\n",
    "Let's see this with examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5b2445",
   "metadata": {},
   "source": [
    "### Inner Join (Default)\n",
    "\n",
    "Only keeps rows where `person_id` exists in BOTH datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fcfa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join - only matching rows\n",
    "merged_inner = pd.merge(\n",
    "    media_df, \n",
    "    demo_df, \n",
    "    on='person_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"Original sizes:\")\n",
    "print(f\"  Media: {len(media_df)} rows\")\n",
    "print(f\"  Demo: {len(demo_df)} rows\")\n",
    "print(f\"\\nAfter inner join: {len(merged_inner)} rows\")\n",
    "print(f\"Columns: {len(merged_inner.columns)}\")\n",
    "\n",
    "merged_inner.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b3b167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all people matched\n",
    "print(f\"Did all media people match? {len(merged_inner) == len(media_df)}\")\n",
    "print(f\"Did all demo people match? {len(merged_inner) == len(demo_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064dbf57",
   "metadata": {},
   "source": [
    "### Left Join\n",
    "\n",
    "Keeps ALL rows from the **left** DataFrame (media_df), adds demo data where available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db397a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join - keep all media rows\n",
    "merged_left = pd.merge(\n",
    "    media_df,\n",
    "    demo_df,\n",
    "    on='person_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"After left join: {len(merged_left)} rows\")\n",
    "print(f\"Same as media_df? {len(merged_left) == len(media_df)}\")\n",
    "\n",
    "merged_left.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b5d177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for unmatched rows (NaN in demo columns)\n",
    "unmatched = merged_left[merged_left['gender'].isna()]\n",
    "print(f\"\\nPeople in media but NOT in demo: {len(unmatched)}\")\n",
    "if len(unmatched) > 0:\n",
    "    print(unmatched.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230d0a95",
   "metadata": {},
   "source": [
    "### Right Join\n",
    "\n",
    "Keeps ALL rows from the **right** DataFrame (demo_df), adds media data where available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607a3aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right join - keep all demo rows\n",
    "merged_right = pd.merge(\n",
    "    media_df,\n",
    "    demo_df,\n",
    "    on='person_id',\n",
    "    how='right'\n",
    ")\n",
    "\n",
    "print(f\"After right join: {len(merged_right)} rows\")\n",
    "print(f\"Same as demo_df? {len(merged_right) == len(demo_df)}\")\n",
    "\n",
    "merged_right.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79d1365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for unmatched rows (NaN in media columns)\n",
    "unmatched_right = merged_right[merged_right['tv_total'].isna()]\n",
    "print(f\"\\nPeople in demo but NOT in media: {len(unmatched_right)}\")\n",
    "if len(unmatched_right) > 0:\n",
    "    print(unmatched_right.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a42198",
   "metadata": {},
   "source": [
    "### Outer Join (Full Join)\n",
    "\n",
    "Keeps ALL rows from BOTH DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087bfbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer join - keep everything\n",
    "merged_outer = pd.merge(\n",
    "    media_df,\n",
    "    demo_df,\n",
    "    on='person_id',\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "print(f\"After outer join: {len(merged_outer)} rows\")\n",
    "print(f\"\\nBreakdown:\")\n",
    "print(f\"  Original media: {len(media_df)}\")\n",
    "print(f\"  Original demo: {len(demo_df)}\")\n",
    "print(f\"  After outer: {len(merged_outer)}\")\n",
    "\n",
    "merged_outer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1489aa",
   "metadata": {},
   "source": [
    "## 4. Detecting Unmatched Rows with Indicator\n",
    "\n",
    "Use `indicator=True` to see which rows matched and which didn't:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c82a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with indicator\n",
    "merged_indicator = pd.merge(\n",
    "    media_df,\n",
    "    demo_df,\n",
    "    on='person_id',\n",
    "    how='outer',\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "# Check the indicator values\n",
    "print(\"Merge indicator counts:\")\n",
    "print(merged_indicator['_merge'].value_counts())\n",
    "print(\"\\nIndicator meanings:\")\n",
    "print(\"  both = matched in both datasets\")\n",
    "print(\"  left_only = in media but not demo\")\n",
    "print(\"  right_only = in demo but not media\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfc9d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View rows that didn't match\n",
    "left_only = merged_indicator[merged_indicator['_merge'] == 'left_only']\n",
    "print(f\"\\nRows only in media: {len(left_only)}\")\n",
    "if len(left_only) > 0:\n",
    "    print(left_only.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d65cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View rows only in demo\n",
    "right_only = merged_indicator[merged_indicator['_merge'] == 'right_only']\n",
    "print(f\"Rows only in demo: {len(right_only)}\")\n",
    "if len(right_only) > 0:\n",
    "    print(right_only.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c9311a",
   "metadata": {},
   "source": [
    "## 5. Handling Duplicate Column Names\n",
    "\n",
    "What if both DataFrames have a column with the same name (other than the merge key)?\n",
    "\n",
    "Let's demonstrate with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4014a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataframes with overlapping column names\n",
    "df1 = pd.DataFrame({\n",
    "    'person_id': [1, 2, 3],\n",
    "    'name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'score': [85, 90, 88]\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'person_id': [1, 2, 4],\n",
    "    'name': ['Alice A.', 'Bob B.', 'David'],\n",
    "    'grade': ['A', 'A', 'B']\n",
    "})\n",
    "\n",
    "print(\"DataFrame 1:\")\n",
    "print(df1)\n",
    "print(\"\\nDataFrame 2:\")\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9a3524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge - notice the 'name' column appears in both\n",
    "merged_dup = pd.merge(df1, df2, on='person_id', how='outer')\n",
    "\n",
    "print(\"\\nAfter merge (default suffixes):\")\n",
    "print(merged_dup)\n",
    "print(f\"\\nColumns: {merged_dup.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ad5d4f",
   "metadata": {},
   "source": [
    "### Custom Suffixes\n",
    "\n",
    "Use `suffixes=` to control how duplicate columns are named:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7b480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use custom suffixes\n",
    "merged_custom = pd.merge(\n",
    "    df1, \n",
    "    df2, \n",
    "    on='person_id', \n",
    "    how='outer',\n",
    "    suffixes=('_test1', '_test2')\n",
    ")\n",
    "\n",
    "print(\"With custom suffixes:\")\n",
    "print(merged_custom)\n",
    "print(f\"\\nColumns: {merged_custom.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b4053b",
   "metadata": {},
   "source": [
    "## 6. Building the Final Merged Dataset\n",
    "\n",
    "For our analysis, we'll create a complete dataset merging media and demographics.\n",
    "\n",
    "We'll use an **inner join** since we only want people who have both media exposure and demographic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601a0f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final merged dataset\n",
    "final_merged = pd.merge(\n",
    "    media_df,\n",
    "    demo_df,\n",
    "    on='person_id',\n",
    "    how='inner',\n",
    "    validate='1:1'  # Ensure one-to-one relationship\n",
    ")\n",
    "\n",
    "print(f\"Final merged dataset: {final_merged.shape}\")\n",
    "print(f\"\\nColumn summary:\")\n",
    "print(f\"  Total columns: {len(final_merged.columns)}\")\n",
    "print(f\"  From media: ~{len(media_df.columns)}\")\n",
    "print(f\"  From demo: ~{len(demo_df.columns)}\")\n",
    "print(f\"  (person_id counted once)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce3f847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the merged data\n",
    "final_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eff0443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "print(\"Data types in merged dataset:\")\n",
    "print(final_merged.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a3573d",
   "metadata": {},
   "source": [
    "### Validate Merge Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4065ab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any missing values introduced by merge\n",
    "print(\"Missing values after merge:\")\n",
    "missing = final_merged.isnull().sum()\n",
    "print(missing[missing > 0])\n",
    "\n",
    "if missing.sum() == 0:\n",
    "    print(\"\\nâœ“ No missing values introduced by merge!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4225474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = final_merged.duplicated(subset='person_id').sum()\n",
    "print(f\"\\nDuplicate person_ids: {duplicates}\")\n",
    "\n",
    "if duplicates == 0:\n",
    "    print(\"âœ“ No duplicate IDs - clean one-to-one merge!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523e630c",
   "metadata": {},
   "source": [
    "## 7. Merge on Multiple Columns\n",
    "\n",
    "Sometimes you need to merge on more than one column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619ce4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: merge on multiple columns\n",
    "# Suppose we had region data too\n",
    "\n",
    "df_region1 = pd.DataFrame({\n",
    "    'person_id': [1, 1, 2, 2],\n",
    "    'year': [2024, 2025, 2024, 2025],\n",
    "    'region': ['North', 'North', 'South', 'South']\n",
    "})\n",
    "\n",
    "df_region2 = pd.DataFrame({\n",
    "    'person_id': [1, 1, 2, 2],\n",
    "    'year': [2024, 2025, 2024, 2025],\n",
    "    'sales': [100, 150, 200, 250]\n",
    "})\n",
    "\n",
    "# Merge on both person_id AND year\n",
    "multi_merge = pd.merge(\n",
    "    df_region1,\n",
    "    df_region2,\n",
    "    on=['person_id', 'year'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(\"Merge on multiple columns:\")\n",
    "print(multi_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efa7ca3",
   "metadata": {},
   "source": [
    "## 8. Merge with Different Column Names\n",
    "\n",
    "Use `left_on=` and `right_on=` when merge keys have different names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6abe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with different column names\n",
    "customers = pd.DataFrame({\n",
    "    'customer_id': [101, 102, 103],\n",
    "    'name': ['Alice', 'Bob', 'Charlie']\n",
    "})\n",
    "\n",
    "orders = pd.DataFrame({\n",
    "    'order_id': [1, 2, 3],\n",
    "    'cust_id': [101, 102, 104],\n",
    "    'amount': [50, 75, 100]\n",
    "})\n",
    "\n",
    "# Merge when key columns have different names\n",
    "diff_merge = pd.merge(\n",
    "    customers,\n",
    "    orders,\n",
    "    left_on='customer_id',\n",
    "    right_on='cust_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(\"Merge with different key names:\")\n",
    "print(diff_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dcc2a4",
   "metadata": {},
   "source": [
    "## 9. Performance Considerations\n",
    "\n",
    "For large datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b807aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check merge performance\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "test_merge = pd.merge(media_df, demo_df, on='person_id', how='inner')\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Merge completed in {elapsed:.4f} seconds\")\n",
    "print(f\"Rows processed: {len(media_df):,}\")\n",
    "print(f\"Merge rate: {len(media_df)/elapsed:,.0f} rows/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20d324e",
   "metadata": {},
   "source": [
    "### Tips for Faster Merges\n",
    "\n",
    "```python\n",
    "# 1. Ensure merge columns are the same data type\n",
    "media_df['person_id'] = media_df['person_id'].astype('int64')\n",
    "demo_df['person_id'] = demo_df['person_id'].astype('int64')\n",
    "\n",
    "# 2. Set index on merge column for repeated merges\n",
    "media_indexed = media_df.set_index('person_id')\n",
    "demo_indexed = demo_df.set_index('person_id')\n",
    "merged = media_indexed.join(demo_indexed, how='inner')\n",
    "\n",
    "# 3. Use categorical dtype for merge keys with few unique values\n",
    "# (Not applicable here, but useful for region, country, etc.)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572974c8",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "âœ… Harmonize column names before merging (`.str.lower()`, `.str.replace()`)  \n",
    "âœ… Use `pd.merge()` with different join types (inner, left, right, outer)  \n",
    "âœ… Understand when to use each join type  \n",
    "âœ… Detect unmatched rows with `indicator=True`  \n",
    "âœ… Handle duplicate column names with `suffixes=`  \n",
    "âœ… Merge on single or multiple columns  \n",
    "âœ… Merge with different column names using `left_on=` and `right_on=`  \n",
    "âœ… Validate merge quality (check duplicates, missing values)  \n",
    "âœ… Build analysis-ready merged datasets\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Always harmonize column names first**: Prevents merge errors\n",
    "2. **Choose the right join type**: Think about your analysis needs\n",
    "3. **Use indicator=True**: Helps detect data quality issues\n",
    "4. **Validate after merging**: Check for duplicates and unexpected nulls\n",
    "5. **Document your merge logic**: Comment why you chose left/inner/outer\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook (**05_groupby_and_aggregation.ipynb**), we'll:\n",
    "- Use this merged dataset for analysis\n",
    "- Group data by demographics\n",
    "- Calculate aggregated statistics\n",
    "- Compute purchase rates by segment\n",
    "- Create summary tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296c627c",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Practice Exercises\n",
    "\n",
    "Try these on your own:\n",
    "\n",
    "1. Create a left join keeping all media rows - how many have missing demo data?\n",
    "2. Merge with `indicator=True` and count rows in each category (both, left_only, right_only)\n",
    "3. Create an outer join and fill missing media columns with 0\n",
    "4. Merge only people aged 25-50 from demo with all media data\n",
    "5. Create a merge that keeps only females with high TV exposure (>30)\n",
    "6. Check if there are any `person_id` values that appear multiple times in either dataset\n",
    "7. Create a custom suffixes merge and rename the suffix columns afterward\n",
    "8. Merge and then filter to only purchasers (`purchase == 1`)\n",
    "\n",
    "### Bonus Challenges\n",
    "\n",
    "9. Create age bands in demo_df first, then merge and analyze purchase rates by age band\n",
    "10. Merge three datasets (media, demo, and a hypothetical third dataset)\n",
    "11. Perform a merge, then groupby a demographic variable to find average TV exposure\n",
    "12. Create a validation function that checks merge quality (duplicates, nulls, row counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5484e43",
   "metadata": {},
   "source": [
    "## Loading/Saving Data Between Notebooks\n",
    "\n",
    "### Load Cleaned Data from Previous Notebook\n",
    "\n",
    "If you cleaned data in notebook 03:\n",
    "\n",
    "```python\n",
    "# Uncomment to load cleaned data\n",
    "# media_df = pd.read_csv('../outputs/media_cleaned.csv')\n",
    "# demo_df = pd.read_csv('../outputs/demo_cleaned.csv')\n",
    "#\n",
    "# # Ensure person_id exists (adjust based on your cleaning)\n",
    "# if 'person_id' not in media_df.columns:\n",
    "#     media_df.columns = media_df.columns.str.lower().str.replace(' ', '_')\n",
    "```\n",
    "\n",
    "### Save Merged Dataset for Next Notebooks\n",
    "\n",
    "Save your merged data for use in analysis notebooks:\n",
    "\n",
    "```python\n",
    "# Uncomment to save merged data\n",
    "# import os\n",
    "# os.makedirs('../outputs', exist_ok=True)\n",
    "#\n",
    "# # Save as CSV\n",
    "# final_merged.to_csv('../outputs/merged_data.csv', index=False)\n",
    "#\n",
    "# # Or save as Parquet (faster, smaller, preserves dtypes)\n",
    "# final_merged.to_parquet('../outputs/merged_data.parquet')\n",
    "#\n",
    "# print(f\"Saved merged dataset: {final_merged.shape}\")\n",
    "```\n",
    "\n",
    "**Tip**: Parquet format is recommended for intermediate files - it's faster to read/write and preserves data types!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
